# Model Evaluation 深度学习模型常用评价指标

- confusion matrix 混淆矩阵
- precision 精确率/查准率
- recall 召回率/查全率
- F1-score
- PRC
- ROC和AUC
- IOU

## confusion matrix, precision, recall, PRC, F1-score

![](https://images2018.cnblogs.com/blog/1093303/201802/1093303-20180224204329655-1160264310.png)

- 第一个字母表示本次预测的正确性，T就是正确，F就是错误；第二个字母则表示由分类器预测的类别，P代表预测为正例，N代表预测为反例。
    - True Positive （真正，TP）被模型预测为正的正样本
    - True Negative（真负 , TN）被模型预测为负的负样本
    - False Positive （假正, FP）被模型预测为正的负样本
    - False Negative（假负 , FN）被模型预测为负的正样本
- Precision指标在中文里可以称为查准率或者是精确率，Recall指标在中卫里常被称为查全率或者是召回率，查准率P和查全率R
- 精确率/查准率(Precision）是指在所有系统判定的“真”的样本中，确实是真的的占比
- 召回率/查全率（Recall）是指在所有确实为真的样本中，被判为“真”的占比
- 准确率是指全部样本中判定正确的占比
- 查准率和查全率是一对矛盾的度量，一般而言，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。我们从直观理解确实如此：我们如果希望好瓜尽可能多地选出来，则可以通过增加选瓜的数量来实现，如果将所有瓜都选上了，那么所有好瓜也必然被选上，但是这样查准率就会越低；若希望选出的瓜中好瓜的比例尽可能高，则只选最有把握的瓜，但这样难免会漏掉不少好瓜，导致查全率较低。通常只有在一些简单任务中，才可能使查全率和查准率都很高。

![](https://images2018.cnblogs.com/blog/1093303/201802/1093303-20180224205202185-367942820.png)

![](https://images2018.cnblogs.com/blog/1093303/201802/1093303-20180224205007882-641098274.png)

- PRC，其全称就是Precision Recall Curve，它以查准率为Y轴，查全率为X轴做的图。它是综合评价整体结果的评估指标。

![](https://images2018.cnblogs.com/blog/1093303/201802/1093303-20180224204341976-647484907.png)

- 上图就是一幅P-R图，它能直观地显示出学习器在样本总体上的查全率和查准率，显然它是一条总体趋势是递减的曲线。在进行比较时，若一个学习器的PR曲线被另一个学习器的曲线完全包住，则可断言后者的性能优于前者，比如上图中A优于C。但是B和A谁更好呢？因为AB两条曲线交叉了，所以很难比较，这时比较合理的判据就是比较PR曲线下的面积，该指标在一定程度上表征了学习器在查准率和查全率上取得相对“双高”的比例。因为这个值不容易估算，所以人们引入“平衡点”(BEP)来度量，他表示“查准率=查全率”时的取值，值越大表明分类器性能越好，以此比较我们一下子就能判断A较B好
- BEP相对比较简化，F1-score更为常用，因为是一个综合考虑了precision和recall的指标

![](https://images2018.cnblogs.com/blog/1093303/201802/1093303-20180224205027586-434414606.png)

## ROC，AUC
- ROC全称是“受试者工作特征”（Receiver Operating Characteristic）曲线，ROC曲线以“真正例率”（TPR）为Y轴，以“假正例率”（FPR）为X轴，对角线对应于“随机猜测”模型，而（0,1）则对应“理想模型”。ROC形式如下图所示。

![](https://images2018.cnblogs.com/blog/1093303/201802/1093303-20180224204355006-1724131879.png)

![](https://images2018.cnblogs.com/blog/1093303/201802/1093303-20180224205043320-355960721.png)

- 从形式上看TPR就是我们上面提到的查全率Recall，而FPR的含义就是：所有确实为“假”的样本中，被误判真的样本。
- 进行学习器比较时，与PR图相似，若一个学习器的ROC曲线被另一个学习器的曲线包住，那么我们可以断言后者性能优于前者；若两个学习器的ROC曲线发生交叉，则难以一般性断言两者孰优孰劣。此时若要进行比较，那么可以比较ROC曲线下的面积，即AUC，面积大的曲线对应的分类器性能更好。
- AUC（Area Under Curve）的值为ROC曲线下面的面积，若分类器的性能极好，则AUC为1。但现实生活中尤其是工业界不会有如此完美的模型，一般AUC均在0.5到1之间，AUC越高，模型的区分能力越好，上图AUC为0.81。若AUC=0.5，即与上图中红线重合，表示模型的区分能力与随机猜测没有差别。若AUC真的小于0.5，可能是好坏标签标反了，或者是模型真的很差。

## IOU

![](https://images2018.cnblogs.com/blog/1093303/201802/1093303-20180224204418589-582088419.png)

![](https://images2018.cnblogs.com/blog/1093303/201802/1093303-20180224204428523-871794088.png)

- IOU数值越大表示该检测器的性能越好


## Reference
- https://www.cnblogs.com/skyfsm/p/8467613.html