# 深度学习优化器

## SGD
- batch gradient descent
    - 批梯度下降，是指所有数据一次性全部喂给模型，然后用梯度下降法更新参数。这种策略的问题就是一次迭代的时间太长了，计算量过大，特别是大数据集
- stochastic gradient descent
    - 随机梯度下降，每次随机选择一个数据，喂给模型，然后更新参数。这种策略的问题是每次数据太少了，模型学不到什么东西。并且容易收敛至局部最优点，特别是山谷和鞍点两类地形。
    ![](https://img-blog.csdnimg.cn/20210301194212506.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0NjE0NTI0,size_16,color_FFFFFF,t_70)
    - 在山谷中，准确的梯度方向是沿山道向下，稍有偏离就会撞向山壁，而粗糙的梯度估计使得它在两山壁间来回反弹震荡，不能沿山道方向迅速下降，导致收敛不稳定和收敛速度慢。在鞍点处，随机梯度下降法会走入一片平坦之地(此时离最低点还很远，故也称plateau)。想象一下蒙着双眼只凭借脚底感觉坡度，如果坡度很明显，那么基本能估计出下山的大致方向；如果坡度不明显，则很可能走错方向。同样，在梯度近乎为零的区域，随机梯度下降法无法准确察觉出梯度的微小变化，结果就停滞下来。
- mini-batch gradient descent
    - 小批梯度下降，是指把训练数据分成很多了mini-batch（也就是很多个数据的集合），每次喂一个mini-batch给模型，然后用梯度下降法来更新参数。
$$g_t = \nabla_{\theta_{t-1}}f(\theta_{t-1})\\  
\Delta\theta_t = -\eta \times g_t$$
- 其中$\eta$是学习率，$g_t$是梯度。SGD完全依赖于当前batch的梯度，所以 可理解为允许当前batch的梯度多大程度影响参数更新 
- 选择合适的learning rate比较困难 - 对所有的参数更新使用同样的learning rate。对于稀疏数据或者特征，有时我们可能想更新快一些对于不经常出现的特征，对于常出现的特征更新慢一些，这时候SGD就不太能满足要求了。同时，SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点。
- 随机梯度下降本质上是采用迭代方式更新参数，每次迭代在当前位置的基础上，沿着某一方向迈一小步抵达下一位置，然后再下一位置重复上述步骤。改造的随机梯度下降法仍然基于这个更新公式。

## 动量 Momentum
$$v_t = \gamma v_{t-1} + \eta g_t\\
\Delta \theta_t = -v_t$$
- 具体来说，前进步伐 $v_{t}$，由两部分组成。一是学习速率 ${\eta}$ 乘以当前估计的梯度 ${g_{t}}$ ；二是带衰减的前一次步伐 ${v_{t-1}}$ 这里，惯性就体现在对前一次步伐信息的重利用上。类比中学物理知识，当前梯度就好比当前时刻受力产生的加速度，前一次步伐好比前一时刻的速度，当前步伐好比当前时刻的速度。为了计算当前时刻的速度，应当考虑前一时刻速度和当前加速度共同作用 的结果，因此 ${v_{t}}$ 直接依赖于 ${v_{t -1}}$ 和 ${g_{t}}$ ，而不仅仅是 ${g_{t}}$ 。另外，衰减系数 ${\gamma}$ 扮演了阻力的作用。
- 特点
    - 下降初期时，使用上一次参数更新，下降方向一致，乘上较大的 $\eta$ 能够进行很好的加速。
    - 下降中后期时，在局部最小值来回震荡的时候，梯度趋于0，使得更新幅度增大，跳出陷阱。
    - 在梯度改变方向的时候，$\gamma$ 能够减少更新.总而言之，momentum项能够在相关方向加速SGD，抑制振荡，从而加快收敛。

## AdaGrad
- 在应用中，我们希望更新频率低的参数可以拥有较大的更新步幅，而更新频率高的参数的步幅可以减小。AdaGrad方法采用“历史梯度平方和”来衡量不同参数的梯度的稀疏性，取值越小表明越稀疏
$$\Delta \theta_t = -\frac{\eta}{\sqrt{\sum_{k=0}^t g_k^2+\epsilon}} g_t$$
- 分母中求和的形式实现了退火过程，这是很多优化技术中常见的策略，意味着随着时间的推移，学习速率(系数)越来越小。从而保证了算法的最终收敛。其中，$\epsilon$ 确保分母不为零。
- 特点
    - 前期放大梯度，后期约束梯度
    - 适合处理稀疏梯度 
- 缺点
    - 仍依赖于人工设置一个全局学习率。学习率设置的不好，梯度调节的就不好
    - 中后期，分母上梯度平方的累加将会越来越大，梯度逐渐趋于0，训练可能提前结束

## Adadelta
- Adadelta是对Adagrad的扩展，最初方案依然是对学习率进行自适应约束，但是进行了计算上的简化。Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值。
- 可以经过近似牛顿迭代法之后，不依赖于全局学习率
- 特点
    - 训练初中期，加速效果不错，很快
    - 训练后期，反复在局部最小值附近抖动

## Adam
- 利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。
- Adam方法将惯性保持和环境感知这两个优点集于一身。 一方面，Adam记录梯度的一阶矩(first moment)，即过往梯度与当前梯度的平均，这体现了惯性保持；另一方面，Adam还记录梯度的二阶矩( sccond moment)，即过往梯度平方与当前梯度平方的平均，这类似AdaGrad方法，体现了环境感知能力，为不同参数产生自适应的学习速率。一阶矩和二阶矩采用类似于滑动窗口内求平均的思想进行融合，即当前梯度和近一段时间内梯度的平均值，时间久远的梯度对当前平均值的贡献呈指数衰减。具体来说，一阶矩和二阶矩采用指数衰退平均(exponential decay average)技术。计算公式如下
$$ m_t = \beta_1m_{t-1} + (1-\beta_1)g_t \\
v_t = \beta_2v_{t-1} + (1-\beta_2)g_t^2$$
- $\beta_1$ 和 $\beta_2$ 为衰减系数，$m_t$是一阶矩，$v_t$是二阶矩
    - $m_t$ 大，$v_t$ 大：梯度大且稳定，这表明遇到一个明显的大坡，前进方向明确
    - $m_t$ 小，$v_t$ 大：梯度不稳定，表明可能遇到一个峡谷，容易引起反弹震荡
    - $m_t$ 大，$v_t$ 小：不可能出现
    - $m_t$ 大，$v_t$ 大：梯度趋于零，可能到达局部最低点
$$ \hat{m_t} = \frac{m_t}{1-\beta_1^t}\\
\hat{v_t} = \frac{v_t}{1-\beta_2^t}\\
\Delta \theta_t = -\frac{\hat{m_t}}{\sqrt{\hat{v_t}}+\epsilon}\eta$$
- 其中，$m_t$ 和 $v_t$ 可以看作 $g_t$ 和 $g_t^2$ 的期望，$\hat{m_t}$ 和 $\hat{v_t}$ 是校正，可以近似为对期望的无偏估计。可以根据梯度进行动态调整，而对学习率形成一个动态约束，而且有明确的范围。
- 特点
    - 结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点
    - 对内存需求较小
    - 为不同的参数计算不同的自适应学习率
    - 也适用于大多非凸优化 - 适用于大数据集和高维空间

## Ranger优化器
  - RAdam + LookAhead的协同组合。
  - RAdam利用动态整流器根据方差调整Adam的自适应动量，并有效地提供自动预热，根据当前数据集定制，以确保扎实的训练开始。LookAhead受到深度神经网络损失表面理解的最新进展的启发，并在整个训练期间提供了稳健和稳定探索的突破。**可以很好的适应小样本数据集**。
  - 效果：训练loss波动振幅明显下降，验证集准确率稳步上升。2021软件杯中，暂未发现最终准确率有提升。
  - 论文地址
    RAdam：https://arxiv.org/abs/1908.03265
    LookAhead：https://arxiv.org/abs/1907.08610v1
  - 项目地址
    RAdam：https://github.com/LiyuanLucasLiu/RAdam


## Reference
[1] [优化器总结](https://mp.weixin.qq.com/s/RB4hh7GHw9-z-O4QlCFMBw)
[2] [SGD CSDN](https://blog.csdn.net/lemon4869/article/details/102156435)
[3] [SGD详解](https://blog.csdn.net/qq_44614524/article/details/114241259)