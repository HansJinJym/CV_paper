# FlowNet
- FlowNet: Learning Optical Flow with Convolutional Networks
- 论文：[ArXiv](https://arxiv.org/abs/1504.06852v2)，[ICCV 2015](https://link.zhihu.com/?target=http%3A//xxx.itp.ac.cn/abs/1504.06852)
- 官方视频Demo：[YouTube](https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3Dk_wkDLJ8lJE)
- 官方开源代码：[code](https://link.zhihu.com/?target=https%3A//lmb.informatik.uni-freiburg.de/resources/software.php)
- 非官方Pytorch实现：[Github](https://link.zhihu.com/?target=https%3A//github.com/ClementPinard/FlowNetPytorch)

### 光流
- 本文是将CNN引入光流估计的开篇之作，构建了能够解决光流估计问题作为监督学习任务的CNN。论文提出并比较了两种体系结构：一种通用体系结构，另一种体系结构加入了一个layer将不同图像位置的特征向量相关联。
- 光流是利用图像序列中像素在时间域上的变化以及相邻帧之间的相关性来找到上一帧跟当前帧之间存在的对应关系，从而计算出相邻帧之间物体的运动信息的一种方法
  ![](https://img-blog.csdn.net/20171007192629122)
- 光流的可视化，左边图片中每个像素都有一个x方向和y方向的位移，右图是通过计算得到的光流flow，是个和原来图像大小相等的双通道图像。不同颜色表示不同的运动方向，深浅表示运动的速度。
  ![](https://img-blog.csdn.net/20171007192932236)
- 光流估计需要精确的逐像素定位，还需要找到两个输入图像之间的对应关系。这不仅需要学习图像特征表示，还需要学习在两个图像中的不同位置进行匹配。
- 现有的数据集由于规模较小无法训练光流网络，因此作者构造了Flying Chairs数据集，该数据集由Flickr的随机背景图像组成，在该图像上叠加了椅子的分割图像。纯人造数据，但是很好地解决了数据量不够的问题（因为椅子可以随意添加）。仅对这些数据进行训练的CNN即可令人惊讶地将其很好地推广到实际数据集，甚至无需进行微调。该网络以Sintel数据集的全分辨率预测每秒最多10个图像对的光流。

### 光流网络
- FlowNet神经网络的大体思路。输入为两张图像，他们分别是第 t 帧以及第 t+1 帧的图像。它们首先通过一个由卷积层组成的收缩部分，用以提取各自的特征图。但是这样会使图片缩小，因此需要再通过一个扩大层，将其扩展到原图大小，进行光流预测。
  ![](https://img-blog.csdn.net/20171007193354633)
- 采用端到端的学习方法来预测光流：给定一个由图像对和GT流组成的数据集，以此训练网络以直接从图像中预测x-y流场。然后需要考虑什么样的网络结构可以解决这个问题。
- 一个简单的选择是将两个输入图像堆叠在一起，并通过一个相当通用的网络将其输入，从而使网络可以自行决定如何处理图像对以提取运动信息，即FlowNetSimple。
  ![](https://img-blog.csdn.net/20171007194106462)
  原则上，如果该网络足够大，则可以学习预测光流量。但是，我们永远不能确定像随机梯度下降这样的局部梯度优化是否可以使网络达到这一点。因此需要手动设计一种通用性较低的架构，在给定的数据和优化技术下可能会表现更好，这将是有益的。
- 另一种方法是为两个图像创建两个单独的但相同的处理流，并在以后的阶段将它们组合起来。通过这种架构，网络必须首先生成两个图像的有意义的表示，然后再将它们组合到更高的层次上，即FlowNetCorr。
  ![](https://img-blog.csdn.net/20171007194512760)
  为了帮助网络进行此匹配过程，论文中引入了一个“相关层”，可以在两个特征图之间执行乘法块的比较。
- 公式1的理解：原图中7x7x3的一部分区域，三层卷积后变为对应的1x1x256。两张特征图各取出正方形的一块，边长为 2k+1 ，进行两个块的卷积运算，因此也没有可训练权重。公式计算复杂度为 c(2k+1)<sup>2</sup> 。
- 反卷积模块。
  ![](https://img-blog.csdn.net/20171007200233349)

### Reference
[1] https://blog.csdn.net/zjc8888888888/article/details/78171018
[2] https://zhuanlan.zhihu.com/p/165110862